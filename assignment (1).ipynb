{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232NSVI4Os05"
      },
      "outputs": [],
      "source": [
        "import torch, torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Input\n",
        "x = torch.randn(1,5,8)\n",
        "\n",
        "# Q K V\n",
        "WQ,WK,WV = torch.randn(8,8),torch.randn(8,8),torch.randn(8,8)\n",
        "Q,K,V = x@WQ, x@WK, x@WV\n",
        "\n",
        "# Scaled Dot-Product Attention\n",
        "attn = F.softmax((Q@K.transpose(-2,-1))/(8**0.5), dim=-1)\n",
        "out = attn@V\n",
        "\n",
        "print(out.shape)\n",
        "\n",
        "# Visualize\n",
        "plt.imshow(attn[0].detach().numpy())\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Linear(d_model, d_model)\n",
        "        self.Wk = nn.Linear(d_model, d_model)\n",
        "        self.Wv = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        Q = self.Wq(x)\n",
        "        K = self.Wk(x)\n",
        "        V = self.Wv(x)\n",
        "\n",
        "        scores = Q @ K.transpose(-2,-1) / (x.size(-1)**0.5)\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "        return out, attn\n",
        "\n",
        "# Example\n",
        "x = torch.randn(1,5,8)        # (batch, seq_len, embed_dim)\n",
        "sa = SelfAttention(8)\n",
        "output, weights = sa(x)\n",
        "\n",
        "print(output.shape)\n"
      ],
      "metadata": {
        "id": "zqeaWLgUOwLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn.functional as F\n",
        "\n",
        "# input\n",
        "x = torch.randn(1,5,8)\n",
        "h, dk = 2, 4\n",
        "\n",
        "# Q K V\n",
        "Wq,Wk,Wv = torch.randn(8,8),torch.randn(8,8),torch.randn(8,8)\n",
        "Q = (x@Wq).view(1,5,h,dk).transpose(1,2)\n",
        "K = (x@Wk).view(1,5,h,dk).transpose(1,2)\n",
        "V = (x@Wv).view(1,5,h,dk).transpose(1,2)\n",
        "\n",
        "# multi-head attention\n",
        "A = F.softmax((Q@K.transpose(-2,-1))/(dk**0.5), dim=-1)\n",
        "heads = A@V\n",
        "\n",
        "print(\"Head1:\", heads[0,0])\n",
        "print(\"Head2:\", heads[0,1])\n"
      ],
      "metadata": {
        "id": "2_zOgvRBPWfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install transformers matplotlib\n",
        "\n",
        "import torch, matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
        "\n",
        "text = \"deep learning uses attention mechanism\"\n",
        "inputs = tok(text, return_tensors=\"pt\")\n",
        "\n",
        "out = model(**inputs)\n",
        "attn = out.attentions[0][0,0].detach()   # layer0, head0\n",
        "\n",
        "plt.imshow(attn)\n",
        "plt.xticks(range(len(inputs.input_ids[0])), tok.convert_ids_to_tokens(inputs.input_ids[0]), rotation=90)\n",
        "plt.yticks(range(len(inputs.input_ids[0])), tok.convert_ids_to_tokens(inputs.input_ids[0]))\n",
        "plt.title(\"Attention Map (Layer0 Head0)\")\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w4mgGrhkPj4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inp = tok(\"attention heads test\", return_tensors=\"pt\")\n",
        "base = model(**inp).logits\n",
        "\n",
        "for h in range(model.config.num_attention_heads):\n",
        "    mask = torch.ones(model.config.num_hidden_layers, model.config.num_attention_heads)\n",
        "    mask[:,h] = 0\n",
        "    diff = (base - model(**inp, head_mask=mask).logits).abs().mean()\n",
        "    print(\"Head\",h,\"impact:\",diff.item())\n"
      ],
      "metadata": {
        "id": "zD2SH1C-PyLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model=64, heads=4, ff=128):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff), nn.ReLU(), nn.Linear(ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a,_ = self.attn(x,x,x)     # self-attention\n",
        "        x = self.norm1(x + a)\n",
        "        f = self.ffn(x)            # feed-forward\n",
        "        x = self.norm2(x + f)\n",
        "        return x\n",
        "\n",
        "# test\n",
        "x = torch.randn(1,10,64)\n",
        "enc = EncoderBlock()\n",
        "print(enc(x).shape)\n"
      ],
      "metadata": {
        "id": "goUGz1LkQJjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinPosEnc(nn.Module):\n",
        "    def __init__(self, d_model, max_len=50):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0,max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0,d_model,2)*(-math.log(10000)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div)\n",
        "        pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x + self.pe[:,:x.size(1)]\n"
      ],
      "metadata": {
        "id": "0paSiDSZQbJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LearnedPosEnc(nn.Module):\n",
        "    def __init__(self, max_len=50, d_model=64):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self,x):\n",
        "        pos = torch.arange(x.size(1)).unsqueeze(0)\n",
        "        return x + self.emb(pos)\n"
      ],
      "metadata": {
        "id": "1z2iJljVQqAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(1,10,64)\n",
        "\n",
        "sin_pe = SinPosEnc(64)\n",
        "learn_pe = LearnedPosEnc()\n",
        "\n",
        "print(\"Sinusoidal:\", sin_pe(x).shape)\n",
        "print(\"Learned:\", learn_pe(x).shape)\n"
      ],
      "metadata": {
        "id": "4d8-zMASQwuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Short Transformer Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model, heads, d_ff):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        a,_ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + a)\n",
        "        f = self.ffn(x)\n",
        "        x = self.norm2(x + f)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "ufFmMDEuTKVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCnhFwvXTLyy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}